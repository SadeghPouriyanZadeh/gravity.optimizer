@article{schneider2019deepobs,
  title={DeepOBS: A deep learning optimizer benchmark suite},
  author={Schneider, Frank and Balles, Lukas and Hennig, Philipp},
  journal={arXiv preprint arXiv:1903.05499},
  year={2019}
}
@article{choi2019empirical,
  title={On empirical comparisons of optimizers for deep learning},
  author={Choi, Dami and Shallue, Christopher J and Nado, Zachary and Lee, Jaehoon and Maddison, Chris J and Dahl, George E},
  journal={arXiv preprint arXiv:1910.05446},
  year={2019}
}
@article{soydaner2020comparison,
  title={A Comparison of Optimization Algorithms for Deep Learning},
  author={Soydaner, Derya},
  journal={International Journal of Pattern Recognition and Artificial Intelligence},
  pages={2052013},
  year={2020},
  publisher={World Scientific Publishing Company}
}
@inproceedings{wilson2017marginal,
  title={The marginal value of adaptive gradient methods in machine learning},
  author={Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={4148--4158},
  year={2017}
}
@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}
@article{hinton2012neural,
  title={Neural networks for machine learning},
  author={Hinton, Geoffrey and Srivastava, Nitsh and Swersky, Kevin},
  journal={Coursera, video lectures},
  volume={264},
  number={1},
  year={2012}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}
@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}
@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}
@article{dozat2016incorporating,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}
@inproceedings{sashank2018convergence,
  title={On the convergence of adam and beyond},
  author={Sashank, J REDDI and Satyen, KALE and Sanjiv, KUMAR},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@incollection{bisong2019google,
  title={Google Colaboratory},
  author={Bisong, Ekaba},
  booktitle={Building Machine Learning and Deep Learning Models on Google Cloud Platform},
  pages={59--64},
  year={2019},
  publisher={Springer}
}
@article{luo2019adaptive,
  title={Adaptive gradient methods with dynamic bound of learning rate},
  author={Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
  journal={arXiv preprint arXiv:1902.09843},
  year={2019}
}
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
@article{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc V},
  journal={arXiv preprint arXiv:1905.11946},
  year={2019}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}
@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}
@incollection{fukushima1982neocognitron,
  title={Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition},
  author={Fukushima, Kunihiko and Miyake, Sei},
  booktitle={Competition and cooperation in neural nets},
  pages={267--285},
  year={1982},
  publisher={Springer}
}
@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={ICML},
  year={2010}
}
@book{halliday2013fundamentals,
  title={Fundamentals of physics},
  author={Halliday, David and Resnick, Robert and Walker, Jearl},
  year={2013},
  publisher={John Wiley \& Sons}
}
@article{you2019does,
  title={How Does Learning Rate Decay Help Modern Neural Networks?},
  author={You, Kaichao and Long, Mingsheng and Wang, Jianmin and Jordan, Michael I},
  year={2019}
}
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}
@incollection{prechelt1998early,
  title={Early stopping-but when?},
  author={Prechelt, Lutz},
  booktitle={Neural Networks: Tricks of the trade},
  pages={55--69},
  year={1998},
  publisher={Springer}
}
@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}
@inproceedings{krogh1992simple,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John A},
  booktitle={Advances in neural information processing systems},
  pages={950--957},
  year={1992}
}
@inproceedings{ng2004feature,
  title={Feature selection, L 1 vs. L 2 regularization, and rotational invariance},
  author={Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={78},
  year={2004}
}
@article{dauphin2015equilibrated,
  title={Equilibrated adaptive learning rates for non-convex optimization},
  author={Dauphin, Yann and De Vries, Harm and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={1504--1512},
  year={2015}
}
@article{hayashi2016eve,
  title={Eve: A gradient based optimization method with locally and globally adaptive learning rates},
  author={Hayashi, Hiroaki and Koushik, Jayanth and Neubig, Graham},
  journal={arXiv preprint arXiv:1611.01505},
  year={2016}
}
@article{huang2018nostalgic,
  title={Nostalgic Adam: Weighting more of the past gradients when designing the adaptive learning rate},
  author={Huang, Haiwen and Wang, Chang and Dong, Bin},
  journal={arXiv preprint arXiv:1805.07557},
  year={2018}
}
@article{luo2019adaptive,
  title={Adaptive gradient methods with dynamic bound of learning rate},
  author={Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
  journal={arXiv preprint arXiv:1902.09843},
  year={2019}
}
@article{mukkamala2017variants,
  title={Variants of rmsprop and adagrad with logarithmic regret bounds},
  author={Mukkamala, Mahesh Chandra and Hein, Matthias},
  journal={arXiv preprint arXiv:1706.05507},
  year={2017}
}
@article{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  journal={arXiv preprint arXiv:1804.04235},
  year={2018}
}
@inproceedings{zaheer2018adaptive,
  title={Adaptive methods for nonconvex optimization},
  author={Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  booktitle={Advances in neural information processing systems},
  pages={9793--9803},
  year={2018}
}
@article{soydaner2020comparison,
  title={A Comparison of Optimization Algorithms for Deep Learning},
  author={Soydaner, Derya},
  journal={International Journal of Pattern Recognition and Artificial Intelligence},
  pages={2052013},
  year={2020},
  publisher={World Scientific Publishing Company}
}
@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}
@article{ngiam2011bmultimodal,
  title={BMultimodal deep learning,[in Proc. 28th Int. Conf},
  author={Ngiam, J and Khosla, A and Kim, M and Nam, J and Lee, H and Ng, A},
  journal={Mach. Learn},
  volume={2},
  year={2011}
}
@article{de2018convergence,
  title={Convergence guarantees for RMSProp and ADAM in non-convex optimization and an empirical comparison to Nesterov acceleration},
  author={De, Soham and Mukherjee, Anirbit and Ullah, Enayat},
  journal={arXiv preprint arXiv:1807.06766},
  year={2018}
}
