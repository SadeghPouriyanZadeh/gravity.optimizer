@article{schneider2019deepobs,
  title={DeepOBS: A deep learning optimizer benchmark suite},
  author={Schneider, Frank and Balles, Lukas and Hennig, Philipp},
  journal={arXiv preprint arXiv:1903.05499},
  year={2019}
}


@article{choi2019empirical,
  title={On empirical comparisons of optimizers for deep learning},
  author={Choi, Dami and Shallue, Christopher J and Nado, Zachary and Lee, Jaehoon and Maddison, Chris J and Dahl, George E},
  journal={arXiv preprint arXiv:1910.05446},
  year={2019}
}


@article{soydaner2020comparison,
  title={A Comparison of Optimization Algorithms for Deep Learning},
  author={Soydaner, Derya},
  journal={International Journal of Pattern Recognition and Artificial Intelligence},
  pages={2052013},
  year={2020},
  publisher={World Scientific Publishing Company}
}


@inproceedings{wilson2017marginal,
  title={The marginal value of adaptive gradient methods in machine learning},
  author={Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={4148--4158},
  year={2017}
}


@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}


@article{hinton2012neural,
  title={Neural networks for machine learning},
  author={Hinton, Geoffrey and Srivastava, Nitsh and Swersky, Kevin},
  journal={Coursera, video lectures},
  volume={264},
  number={1},
  year={2012}
}


@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}


@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}


@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}


@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}


@article{dozat2016incorporating,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}


@inproceedings{sashank2018convergence,
  title={On the convergence of adam and beyond},
  author={Sashank, J REDDI and Satyen, KALE and Sanjiv, KUMAR},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@incollection{bisong2019google,
  title={Google Colaboratory},
  author={Bisong, Ekaba},
  booktitle={Building Machine Learning and Deep Learning Models on Google Cloud Platform},
  pages={59--64},
  year={2019},
  publisher={Springer}
}